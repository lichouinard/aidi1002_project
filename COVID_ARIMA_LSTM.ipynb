{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand\n",
    "\n",
    "import cmdstanpy\n",
    "import arviz as az\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num, num2date\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from pmdarima import auto_arima #!pip install pmdarima\n",
    "import pmdarima as pm\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import stats as sps\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.stats\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from datetime import timedelta\n",
    "from time import time\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   :  8}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "mpl.rcParams['axes.grid']=True\n",
    "plt.rcParams.update({'figure.figsize':(9, 7), 'figure.dpi':120})\n",
    "\n",
    "mpl.rcParams['axes.grid']=True\n",
    "pd.options.display.max_rows = 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b833ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## Import Data #########################################################\n",
    "confirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "death_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "recovered_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv')\n",
    "\n",
    "\n",
    "def prepare(df, name):\n",
    "    cols_to_melt = df.columns.values[4:]\n",
    "    res = pd.melt(df, id_vars='Country/Region', var_name='date', value_vars=cols_to_melt, value_name=name)\n",
    "    res['date'] = pd.to_datetime(res['date'])\n",
    "    res = res.sort_values(by = ['Country/Region', 'date'])\n",
    "    res = res.set_index('date')\n",
    "    res.columns = ['country', name]\n",
    "    return res\n",
    "\n",
    "confirmed_df = prepare(confirmed_df, 'Confirmed Cases')\n",
    "recovered_df = prepare(recovered_df, 'Recovered Cases')\n",
    "death_df = prepare(death_df, 'Death Cases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1189a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ Plot ##############################################################\n",
    "############################################################ Plot - LSTM ########################################################\n",
    "def plot2series(df, split, col = 'Confirmed Cases', title = ' ', save_name = 'img'):\n",
    "    df1 = df.iloc[0:split, :]\n",
    "    df2 = df.iloc[split - 1:,:]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    \n",
    "    \n",
    "    plt.plot(df1[col], color = 'red', linewidth=2, markersize=6, label = 'Actual Data')\n",
    "    plt.plot(df2[col], color = 'green', linewidth=2, markersize=6, label = 'Predicted Data')\n",
    "\n",
    "    ci = 1.96 * df2[col].std()/df2[col].mean()\n",
    "\n",
    "    plt.fill_between(df2.index.get_level_values('date'), \n",
    "                    (df2[col].values-(df2[col].values * ci)), \n",
    "                    (df2[col].values+(df2[col].values * ci)), \n",
    "                    color='b', alpha=.1, label = 'Confidence interval 95%')\n",
    "\n",
    "    # plt.plot(df1[col], color = 'red', marker='o', linewidth=2, markersize=6, label = 'Actual Data')\n",
    "    # plt.plot(df2[col], color = 'green', marker='o', linewidth=2, markersize=6, label = 'Predicted Data')\n",
    "\n",
    "    x_ticks = np.linspace(df1.index.min().value, df2.index.max().value, 5)\n",
    "    x_ticks = pd.to_datetime(x_ticks)\n",
    "    plt.xticks(x_ticks, rotation = 0)\n",
    "    y_ticks = np.linspace(df1[col].min(), df2[col].max() + (df2[col].max()*0.45), 10) # modifier here\n",
    "    plt.yticks(y_ticks)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    # plt.title(title)\n",
    "    # ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e313ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Plot Rt ##################################################\n",
    "def plot_rt(result, ax, state_name):\n",
    "    # ax.set_title(f\"{state_name}\")\n",
    "    # Colors\n",
    "    ABOVE = [1,0,0]\n",
    "    MIDDLE = [1,1,1]\n",
    "    BELOW = [0,0,0]\n",
    "    cmap = ListedColormap(np.r_[\n",
    "        np.linspace(BELOW,MIDDLE,25),\n",
    "        np.linspace(MIDDLE,ABOVE,25)\n",
    "    ])\n",
    "    color_mapped = lambda y: np.clip(y, .5, 1.5)-.5\n",
    "    \n",
    "    index = result['ML'].index.get_level_values('date')\n",
    "    values = result['ML'].values\n",
    "    \n",
    "    # Plot dots and line\n",
    "    ax.plot(index, values, c='k', zorder=1, alpha=.25)\n",
    "    ax.scatter(index,\n",
    "               values,\n",
    "               s=40,\n",
    "               lw=.5,\n",
    "               c=cmap(color_mapped(values)),\n",
    "               edgecolors='k', zorder=2)\n",
    "    \n",
    "    # Aesthetically, extrapolate credible interval by 1 day either side\n",
    "    lowfn = interp1d(date2num(index),\n",
    "                     result['Low'].values,\n",
    "                     bounds_error=False,\n",
    "                     fill_value='extrapolate')\n",
    "    \n",
    "    highfn = interp1d(date2num(index),\n",
    "                      result['High'].values,\n",
    "                      bounds_error=False,\n",
    "                      fill_value='extrapolate')\n",
    "    \n",
    "    '''Edit'''\n",
    "    extended = pd.date_range(start=pd.Timestamp('2020-08-07'),\n",
    "                             end=index[-1]+pd.Timedelta(days=1))\n",
    "    '''Edit'''\n",
    "    \n",
    "    ax.fill_between(extended,\n",
    "                    lowfn(date2num(extended)),\n",
    "                    highfn(date2num(extended)),\n",
    "                    color='k',\n",
    "                    alpha=.1,\n",
    "                    lw=0,\n",
    "                    zorder=3)\n",
    "\n",
    "    ax.axhline(1.0, c='k', lw=1, label='$R_t=1.0$', alpha=.25);\n",
    "    \n",
    "    # Formatting\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    \n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.1f}\"))\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.margins(0)\n",
    "    ax.grid(which='major', axis='y', c='k', alpha=.1, zorder=-2)\n",
    "    ax.margins(0)\n",
    "    ax.set_ylim(0.0,3.5)\n",
    "\n",
    "    '''Edit'''\n",
    "    ax.set_xlim(pd.Timestamp('2020-08-07'), result.index.get_level_values('date')[-1]+pd.Timedelta(days=1))\n",
    "    '''Edit'''\n",
    "\n",
    "    fig.set_facecolor('w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### ARIMA - LSTM functions ####################################################\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "def mean_absolute_percentage_error(actual, prediction):\n",
    "    return 100 * np.mean(np.abs((actual - prediction))/actual)\n",
    "\n",
    "def get_best_arima(data, train_rate = 0.8):\n",
    "    # prepare train and test data\n",
    "    train_size = int(len(data) * train_rate)\n",
    "    test_size = data.size - train_size \n",
    "    data = data.reset_index(drop=True)\n",
    "    train = data.head(train_size).values.tolist()\n",
    "    test = data.tail(test_size).values.tolist()\n",
    "\n",
    "    # Initialize model\n",
    "    model = auto_arima(train, max_p=3, max_q=3, seasonal=False, trace=True,\n",
    "                       error_action='ignore', suppress_warnings=True)\n",
    "\n",
    "    # Determine model parameters\n",
    "    model.fit(train)\n",
    "    order = model.get_params()['order']\n",
    "    # print('ARIMA order:', order, '\\n')\n",
    "    print(model.summary())\n",
    "    # Genereate predictions\n",
    "    prediction = []\n",
    "    for i in range(len(test)):\n",
    "        model = pm.ARIMA(order=order)\n",
    "        model.fit(train)\n",
    "        # print('working on', i+1, 'of', test_size, '-- ' + str(int(100 * (i + 1) / test_size)) + '% complete')\n",
    "        prediction.append(model.predict()[0])\n",
    "        train.append(test[i])\n",
    "\n",
    "    # Generate error data\n",
    "    mse = mean_squared_error(test, prediction)\n",
    "    rmse = mse ** 0.5\n",
    "    msle = mean_squared_log_error(np.array(test), np.array(prediction))\n",
    "    rmsle = np.sqrt(msle)\n",
    "    mae = mean_absolute_error(np.array(test), np.array(prediction))\n",
    "    mape = mean_absolute_percentage_error(np.array(test), np.array(prediction))\n",
    "    return prediction, order, mse, rmse, msle, rmsle, mae, mape\n",
    "\n",
    "def show_arima(data, train_rate = 0.8, show_plots = False, days_num = 90, Category = 'Confirmed Cases'):\n",
    "    res_arima_test, best_order, mse, rmse, msle, rmsle, mae, mape= get_best_arima(data, train_rate)\n",
    "    print('>>> ', Category, ' ARIMA Best Order : ', best_order, 'MSE = %.5f RMSE = %.5f MSLE = %.5f RMSLE = %.5f MAE = %.5f MAPE = %.5f' % (mse, rmse, msle, rmsle, mae, mape))\n",
    "    model_cc = ARIMA(data[Category], order = best_order, freq = 'D')\n",
    "    fitted_model_cc = model_cc.fit(disp = 0)\n",
    "    residuals_cc = pd.DataFrame(fitted_model_cc.resid)\n",
    "\n",
    "    if show_plots:\n",
    "        fitted_model_cc.plot_predict(best_order[1], len(data) + days_num)\n",
    "        residuals_cc.plot(title= Category + ' Residuals', legend=None)\n",
    "        residuals_cc.plot(kind='kde', title= Category + ' Density', legend=None)\n",
    "        plt.show()\n",
    "\n",
    "    data = data.reset_index(drop=True)\n",
    "    train = data.head(len(data)).values.tolist()\n",
    "    res_ConfirmedCases = []\n",
    "    model = pm.ARIMA(order=best_order)\n",
    "    model.fit(train)\n",
    "    res_ConfirmedCases = model.predict(n_periods=days_num)\n",
    "        \n",
    "    return res_ConfirmedCases\n",
    "\n",
    "def prepare_arima_newCases(df, days_num, date):\n",
    "    df = df + np.random.randint(low = -5000, high = 5000, size = len(df))\n",
    "    NewCases_arima_df = pd.DataFrame({'target':df})\n",
    "\n",
    "    date_idx = pd.date_range(date, periods=len(df), freq='D')\n",
    "    idx = pd.DatetimeIndex(date_idx)\n",
    "    NewCases_arima_df['date'] = idx\n",
    "    NewCases_arima_df = NewCases_arima_df.set_index(['date'])\n",
    "\n",
    "    NewCases_arima_df = NewCases_arima_df[NewCases_arima_df['target']!=0]\n",
    "    NewCases_arima_df = NewCases_arima_df['target'].astype('float64')\n",
    "    morocco = pd.Series(dtype='float64')\n",
    "    morocco = NewCases_arima_df\n",
    "    return morocco\n",
    "\n",
    "def evaluate_lstm(data, train_rate = 0.8, lstm_len=4, Category = 'Confirmed Cases'):\n",
    "    # prepare train and test data\n",
    "    train_size = int(len(data) * train_rate)\n",
    "    test_size = data.size - train_size\n",
    "    val = data.iloc[-test_size:,:] \n",
    "    data = data.reset_index(drop=True)\n",
    "    test = data.tail(test_size).values.tolist()\n",
    "    dataset = np.reshape(data.values, (len(data), 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "\n",
    "    for i in range(lstm_len, train_size):\n",
    "        x_train.append(dataset_scaled[i - lstm_len:i, 0])\n",
    "        y_train.append(dataset_scaled[i, 0])\n",
    "    for i in range(train_size, len(dataset_scaled)):\n",
    "        x_test.append(dataset_scaled[i - lstm_len:i, 0])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # Set up & fit LSTM RNN\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_len, input_shape=(x_train.shape[1], 1)))\n",
    "    model.add(Dense(lstm_len, activation=\"relu\"))\n",
    "    model.add(Dense(int(lstm_len/2), activation=\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    # Compile the model\n",
    "    model.compile(loss=tf.keras.losses.mse, optimizer='adagrad', metrics=[\"mse\"])\n",
    "    early_stopping = EarlyStopping(monitor='loss', mode='min', verbose=0, patience=5)\n",
    "    history = model.fit(x_train, y_train, epochs=20, batch_size=8, verbose=0, callbacks=[early_stopping])\n",
    "    # Show train loss results \n",
    "    print(model.summary())\n",
    "    plt.plot(history.history['loss'], 'b', label='Training loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title(\"Losses during training\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Generate predictions\n",
    "    prediction = model.predict(x_test)\n",
    "    prediction = scaler.inverse_transform(prediction).tolist()\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(prediction)):\n",
    "        output.extend(prediction[i])\n",
    "    prediction = output\n",
    "    # Fitting Results \n",
    "    \n",
    "    plt.plot(val[Category], label='Real Data')\n",
    "    plt.plot(val.index, prediction, label='LSTM test')\n",
    "    x_ticks = np.linspace(val.index.min().value, val.index.max().value, 5)\n",
    "    x_ticks = pd.to_datetime(x_ticks)\n",
    "    plt.xticks(x_ticks, rotation = 0)\n",
    "    y_ticks = np.linspace(0, val[Category].max(), 10)\n",
    "    plt.yticks(y_ticks)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    # Generate error data\n",
    "    mse = mean_squared_error(data.tail(len(prediction)).values, prediction)\n",
    "    rmse = mse ** 0.5\n",
    "    msle = mean_squared_log_error(data.tail(len(prediction)).values, prediction)\n",
    "    rmsle = np.sqrt(msle)\n",
    "    mae = mean_absolute_error(data.tail(len(prediction)).values, prediction)\n",
    "    mape = mean_absolute_percentage_error(np.array(data.tail(len(prediction)).values), np.array(prediction))\n",
    "    return prediction, mse, rmse, msle, rmsle, mae, mape\n",
    "\n",
    "def series_to_supervised(df, n_in=1, n_out=1, Category = 'Confirmed Cases' , dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "\t    cols.append(df.shift(i))\n",
    "\t    names += [Category+'(t-%d)' % i]\n",
    "    for i in range(0, n_out):\n",
    "\t    cols.append(df.shift(-i))\n",
    "\t    if i == 0:\n",
    "\t\t    names += [Category]\n",
    "\t    else:\n",
    "\t\t    names += [Category+'(t-%d)' % i]\n",
    "\t    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "\t    agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def normalize(df, Category = 'Confirmed Cases'):\n",
    "    ndf = df.copy()\n",
    "    min_target = ndf[Category].min()\n",
    "    max_target = ndf[Category].max()\n",
    "    ndf[Category]  = (ndf[Category] - min_target) / (max_target - min_target)\n",
    "    return (ndf, min_target, max_target)\n",
    "\n",
    "def denomalize(df, min_target, max_target, Category = 'Confirmed Cases'):\n",
    "    ddf = df.copy()\n",
    "    ddf[Category] = ddf[Category] * (max_target - min_target) + min_target \n",
    "    return ddf\n",
    "\n",
    "def denomalize_value(value, min_target, max_target):\n",
    "    return value * (max_target - min_target) + min_target\n",
    "\n",
    "def show_lstm(df, train_rate = 0.8, lstm_len=4, days_num = 90, Category = 'Confirmed Cases'):\n",
    "    forcast = df.copy()\n",
    "    train_size = int(len(forcast) * train_rate)\n",
    "    for itr in range(days_num):\n",
    "        ## Encode data\n",
    "        ndf, min_target, max_target = normalize(forcast, Category)\n",
    "        reframed = series_to_supervised(ndf, train_size, 1, Category, True)\n",
    "        train = reframed.values\n",
    "        train_X, train_y = train[:, :-1], train[:, -1]\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "        tf.keras.backend.clear_session()\n",
    "        input_shape=(train_X.shape[1], train_X.shape[2])\n",
    "\n",
    "        # Set up & fit LSTM RNN\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(lstm_len, input_shape=input_shape))\n",
    "        model.add(Dense(lstm_len, activation=\"relu\"))\n",
    "        model.add(Dense(int(lstm_len/2), activation=\"relu\"))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.compile(loss=tf.keras.losses.mse,\n",
    "                    optimizer='adagrad',\n",
    "                    metrics=[\"mse\"])\n",
    "        ## fitting results\n",
    "        model.fit(train_X, train_y, epochs=20, batch_size=8, verbose=0)\n",
    "        ## predict new values \n",
    "        yhat = model.predict(train_X)[-1, 0]\n",
    "        yhat = denomalize_value(yhat, min_target, max_target)\n",
    "        last_date = forcast.iloc[[-1]].index\n",
    "        last_date = last_date + timedelta(days=1)\n",
    "#        print('In', last_date.values, 'Cases:', yhat )\n",
    "        #forcast = forcast.append(pd.DataFrame(yhat, index=last_date, columns = [Category]))\n",
    "        forcast = pd.concat([forcast,pd.DataFrame(yhat, index=last_date, columns = [Category])])\n",
    "    return forcast\n",
    "\n",
    "def prepare_lstm_newCases(df, split):\n",
    "    test = df.copy()\n",
    "    # for i in range(0,len(test)):\n",
    "    #     if(i == 0):\n",
    "    #         n = 0\n",
    "    #     else:\n",
    "    #         n = df[df.columns.values[0]][i-1]\n",
    "    #     v = test[df.columns.values[0]][i] - n\n",
    "    #     if(v > 0):\n",
    "    #         test[df.columns.values[0]][i] = v\n",
    "    #     else:\n",
    "    #         test[df.columns.values[0]][i] = rand.uniform(200,300)\n",
    "    NewCases_df = test.iloc[split -1:,:]\n",
    "    NewCases_df = NewCases_df[df.columns.values[0]].astype('float64')\n",
    "    return NewCases_df\n",
    "####################################################### R effective functions ####################################################\n",
    "def prepare_cases(cases):\n",
    "    new_cases = cases.diff()\n",
    "\n",
    "    smoothed = new_cases.rolling(10,\n",
    "        win_type='gaussian',\n",
    "        min_periods=1,\n",
    "        center=True).mean(std=2).round()\n",
    "    \n",
    "    zeros = smoothed.index[smoothed.eq(0)]\n",
    "    if len(zeros) == 0:\n",
    "        idx_start = 0\n",
    "    else:\n",
    "        last_zero = zeros.max()\n",
    "        idx_start = smoothed.index.get_loc(last_zero) + 1\n",
    "    smoothed = smoothed.iloc[idx_start:]\n",
    "    original = new_cases.loc[smoothed.index]\n",
    "    \n",
    "    return original, smoothed\n",
    "\n",
    "def get_posteriors(sr, window=7, min_periods=1):\n",
    "    lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))\n",
    "\n",
    "    # Note: if you want to have a Uniform prior you can use the following line instead.\n",
    "    # I chose the gamma distribution because of our prior knowledge of the likely value\n",
    "    # of R_t.\n",
    "    \n",
    "    # prior0 = np.full(len(r_t_range), np.log(1/len(r_t_range)))\n",
    "    prior0 = np.log(sps.gamma(a=3).pdf(r_t_range) + 1e-14)\n",
    "\n",
    "    likelihoods = pd.DataFrame(\n",
    "        # Short-hand way of concatenating the prior and likelihoods\n",
    "        data = np.c_[prior0, sps.poisson.logpmf(sr[1:].values, lam)],\n",
    "        index = r_t_range,\n",
    "        columns = sr.index)\n",
    "\n",
    "    # Perform a rolling sum of log likelihoods. This is the equivalent\n",
    "    # of multiplying the original distributions. Exponentiate to move\n",
    "    # out of log.\n",
    "    posteriors = likelihoods.rolling(window,\n",
    "                                     axis=1,\n",
    "                                     min_periods=min_periods).sum()\n",
    "    posteriors = np.exp(posteriors)\n",
    "\n",
    "    # Normalize to 1.0\n",
    "    posteriors = posteriors.div(posteriors.sum(axis=0), axis=1)\n",
    "    \n",
    "    return posteriors\n",
    "\n",
    "def highest_density_interval(pmf, p=.95):\n",
    "    # If we pass a DataFrame, just call this recursively on the columns\n",
    "    if(isinstance(pmf, pd.DataFrame)):\n",
    "        return pd.DataFrame([highest_density_interval(pmf[col]) for col in pmf],\n",
    "                            index=pmf.columns)\n",
    "    \n",
    "    cumsum = np.cumsum(pmf.values)\n",
    "    best = None\n",
    "    for i, value in enumerate(cumsum):\n",
    "        for j, high_value in enumerate(cumsum[i+1:]):\n",
    "            if (high_value-value > p) and (not best or j<best[1]-best[0]):\n",
    "                best = (i, i+j+1)\n",
    "                break\n",
    "    if best:        \n",
    "      low = pmf.index[best[0]]\n",
    "      high = pmf.index[best[1]]\n",
    "    else:\n",
    "      low = pmf.index[0]\n",
    "      high = pmf.index[1]\n",
    "\n",
    "    return pd.Series([low, high], index=['Low', 'High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\" Simulations \"###############################################################\n",
    "#########################################################\"Parameters\"##################################################################\n",
    "days_num = 60 #Number of days to forecast (90 days = 3 months)\n",
    "train_rate = 0.8 # Learning Rate 0.8\n",
    "lstm_size = 64 # Number of LSTM Nodes\n",
    "# Column vector of k\n",
    "k = np.arange(0, 70)[:, None]\n",
    "# Different values of Lambda\n",
    "lambdas = [10, 20, 30, 40]\n",
    "# Evaluated the Probability Mass Function (remember: poisson is discrete)\n",
    "y = sps.poisson.pmf(k, lambdas)\n",
    "k = np.array([20, 40, 55, 90])\n",
    "# We create an array for every possible value of Rt\n",
    "R_T_MAX = 12\n",
    "r_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)\n",
    "# Gamma is 1/serial interval\n",
    "# https://wwwnc.cdc.gov/eid/article/26/7/20-0282_article\n",
    "# https://www.nejm.org/doi/full/10.1056/NEJMoa2001316\n",
    "GAMMA = 1/4\n",
    "# Map Rt into lambda so we can substitute it into the equation below\n",
    "# Note that we have N-1 lambdas because on the first day of an outbreak\n",
    "# you do not know what to expect.\n",
    "lam = k[:-1] * np.exp(GAMMA * (r_t_range[:, None] - 1))\n",
    "# Evaluate the likelihood on each day and normalize sum of each day to 1.0\n",
    "likelihood_r_t = sps.poisson.pmf(k[1:], lam)\n",
    "likelihood_r_t /= np.sum(likelihood_r_t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################### ARIMA #######################################################################\n",
    "# Forecasting Confirmed, Recovered and Death Cases in Morocco\n",
    "country =  'Morocco'  ## you can use any other country name For Example : Brazil, Italy, Spain\n",
    "# # Confirmed Cases\n",
    "country_df = confirmed_df[confirmed_df['country'] == country] ## Choose the right columns for the choosing country\n",
    "country_df = country_df.drop(columns=['country']) ## Drop the columns name, keeping only data\n",
    "country_df = country_df[:'2022-02-22']\n",
    "forcast_ConfirmedCases = country_df.copy() ## Copy the data in forcast variable\n",
    "# # Show ARIMA Confirmed Cases Results \n",
    "#res_arima_ConfirmedCases = show_arima(forcast_ConfirmedCases, train_rate, True, days_num,forcast_ConfirmedCases.columns.values[0])\n",
    "\n",
    "# # Death Cases \n",
    "# country_df = death_df[death_df['country'] == country] ## Choose the right columns for the choosing country\n",
    "# country_df = country_df.drop(columns=['country']) ## Drop the columns name, keeping only data\n",
    "# forcast_DeathCases = country_df.copy() ## Copy the data in forcast variable\n",
    "# # Show ARIMA Death Cases Results \n",
    "# res_arima_DeathCases = show_arima(forcast_DeathCases, train_rate, True, days_num,forcast_DeathCases.columns.values[0])\n",
    "\n",
    "######################################################### LSTM - RNN #######################################################################\n",
    "split = len(forcast_ConfirmedCases)\n",
    "ax = plt.gca()\n",
    "forcast_ConfirmedCases['2020-01-22':'2020-11-22'].plot(kind='line', y='Confirmed Cases', color ='g', ax=ax, use_index = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ca756",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################### Show Results #####################################################################\n",
    "# Confirmed Cases\n",
    "# Metrics\n",
    "res_lstm_ConfirmedCases, mse, rmse, msle, rmsle, mae, mape = evaluate_lstm(forcast_ConfirmedCases,train_rate,lstm_size)\n",
    "print('>>>', forcast_ConfirmedCases.columns.values[0], 'LSTM Results MSE = %.5f RMSE = %.5f MSLE = %.5f RMSLE = %.5f MAE = %.5f MAPE = %.5f' % (mse, rmse, msle, rmsle, mae, mape))\n",
    "# Results\n",
    "res_lstm_ConfirmedCases = show_lstm(forcast_ConfirmedCases, train_rate, lstm_size, days_num, forcast_ConfirmedCases.columns.values[0])\n",
    "split = len(forcast_ConfirmedCases)\n",
    "plot2series(res_lstm_ConfirmedCases, split, forcast_ConfirmedCases.columns.values[0]) ## Plot results \n",
    "\n",
    "# # Death Cases\n",
    "# # Metrics\n",
    "# res_lstm_DeathCases, mse, rmse, msle, rmsle, mae, mape = evaluate_lstm(forcast_DeathCases, train_rate, lstm_size, forcast_DeathCases.columns.values[0])\n",
    "# print('>>>', forcast_DeathCases.columns.values[0], 'LSTM Results MSE = %.5f RMSE = %.5f MSLE = %.5f RMSLE = %.5f MAE = %.5f MAPE = %.5f' % (mse, rmse, msle, rmsle, mae, mape))\n",
    "# # Results\n",
    "# res_lstm_DeathCases = show_lstm(forcast_DeathCases, train_rate, lstm_size, days_num, forcast_DeathCases.columns.values[0])\n",
    "# plot2series(res_lstm_DeathCases, split, forcast_DeathCases.columns.values[0]) ## Plot results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac90a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### Plot R effective #################################################################\n",
    "####################################################### Plot Rt for ARIMA #################################################################\n",
    "# # # ARIMA Prepare Data\n",
    "# NewCases_arima_df = prepare_arima_newCases(res_arima_ConfirmedCases, days_num, '2020-11-23')\n",
    "# # ## ARIMA Plot Rt\n",
    "# original, smoothed = prepare_cases(NewCases_arima_df)\n",
    "# posteriors = get_posteriors(smoothed)\n",
    "# hdis = highest_density_interval(posteriors)\n",
    "# most_likely = posteriors.idxmax().rename('ML')\n",
    "# result = pd.concat([most_likely, hdis], axis=1)\n",
    "# fig, ax = plt.subplots(figsize=(600/72,400/72))\n",
    "# plot_rt(result, ax, country)\n",
    "# ax.set_ylim(0.0,3.5)\n",
    "# ax.xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95283b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### Plot Rt for LSTM #################################################################\n",
    "## LSTM Prepare Data\n",
    "NewCases_lstm_df = prepare_lstm_newCases(res_lstm_ConfirmedCases, split)\n",
    "## LSTM Plot Rt\n",
    "original, smoothed = prepare_cases(NewCases_lstm_df)\n",
    "posteriors = get_posteriors(smoothed)\n",
    "hdis = highest_density_interval(posteriors)\n",
    "most_likely = posteriors.idxmax().rename('ML')\n",
    "result = pd.concat([most_likely, hdis], axis=1)\n",
    "fig, ax = plt.subplots(figsize=(600/72,400/72))\n",
    "plot_rt(result, ax, country)\n",
    "ax.set_ylim(0.0,3.5)\n",
    "ax.xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
